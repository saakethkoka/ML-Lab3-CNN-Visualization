{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 00:32:44.474850: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-21 00:32:44.977506: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/include:/usr/local/cuda-11.0/lib64:\n",
      "2023-03-21 00:32:44.977542: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/include:/usr/local/cuda-11.0/lib64:\n",
      "2023-03-21 00:32:44.977545: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "We decided to use the VGG19 model trained on imagenet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"data/ILSVRC2012_img_val\"\n",
    "GROUND_TRUTH = \"data/ground_truth.txt\"\n",
    "PICKLE_DIR = \"pickles\"\n",
    "file_names = os.listdir(DATA_DIR)\n",
    "READ_FROM_PICKLE = True\n",
    "file_names.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(file_names, data_dir):\n",
    "    images = []\n",
    "    i = 0\n",
    "    for file_name in file_names:\n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            print(\"Read {} images\".format(i))\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        image = Image.open(file_path)\n",
    "        image = image.resize((224, 224))\n",
    "        image = np.array(image)\n",
    "        images.append(image)\n",
    "    return images\n",
    "\n",
    "if not READ_FROM_PICKLE:\n",
    "    images = read_images(file_names, DATA_DIR)\n",
    "    for i in range(len(images)):\n",
    "        # make sure each image has 3 channels\n",
    "        if len(images[i].shape) != 3:\n",
    "            images[i] = np.stack((images[i],)*3, axis=-1)\n",
    "        if images[i].shape[2] == 4:\n",
    "            images[i] = images[i][:,:,:3]\n",
    "    images = np.array(images)\n",
    "    with open(os.path.join(PICKLE_DIR, \"images.pickle\"), \"wb\") as f:\n",
    "        pickle.dump(images, f)\n",
    "else:\n",
    "    with open(os.path.join(PICKLE_DIR, \"images.pickle\"), \"rb\") as f:\n",
    "        images = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 224, 224, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(GROUND_TRUTH, sep=\" \", header=None).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = tf.keras.applications.VGG19(weights='imagenet', input_shape=(224, 224, 3))\n",
    "\n",
    "\n",
    "# make all layers except the last one non-trainable\n",
    "for layer in vgg_model.layers[:-1]:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " fc1 (Dense)                 (None, 4096)              102764544 \n",
      "                                                                 \n",
      " fc2 (Dense)                 (None, 4096)              16781312  \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 1000)              4097000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 143,667,240\n",
      "Trainable params: 4,097,000\n",
      "Non-trainable params: 139,570,240\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# get classes from model:\n",
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1000 is out of bounds for axis 1 with size 1000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/skoka/Documents/MachineLearningPython/ML-Lab3-CNN-Visualization/main.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/skoka/Documents/MachineLearningPython/ML-Lab3-CNN-Visualization/main.ipynb#Y112sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vgg_model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/skoka/Documents/MachineLearningPython/ML-Lab3-CNN-Visualization/main.ipynb#Y112sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                 loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/skoka/Documents/MachineLearningPython/ML-Lab3-CNN-Visualization/main.ipynb#Y112sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                 metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/skoka/Documents/MachineLearningPython/ML-Lab3-CNN-Visualization/main.ipynb#Y112sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# use cpu:\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/skoka/Documents/MachineLearningPython/ML-Lab3-CNN-Visualization/main.ipynb#Y112sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/skoka/Documents/MachineLearningPython/ML-Lab3-CNN-Visualization/main.ipynb#Y112sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# make labels one-hot encoded\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/skoka/Documents/MachineLearningPython/ML-Lab3-CNN-Visualization/main.ipynb#Y112sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m labels \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mkeras\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mto_categorical(labels\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), num_classes\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/skoka/Documents/MachineLearningPython/ML-Lab3-CNN-Visualization/main.ipynb#Y112sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39m/cpu:0\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/skoka/Documents/MachineLearningPython/ML-Lab3-CNN-Visualization/main.ipynb#Y112sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     vgg_model\u001b[39m.\u001b[39mfit(images, labels, epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/np_utils.py:73\u001b[0m, in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     71\u001b[0m n \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m     72\u001b[0m categorical \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((n, num_classes), dtype\u001b[39m=\u001b[39mdtype)\n\u001b[0;32m---> 73\u001b[0m categorical[np\u001b[39m.\u001b[39marange(n), y] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     74\u001b[0m output_shape \u001b[39m=\u001b[39m input_shape \u001b[39m+\u001b[39m (num_classes,)\n\u001b[1;32m     75\u001b[0m categorical \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(categorical, output_shape)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1000 is out of bounds for axis 1 with size 1000"
     ]
    }
   ],
   "source": [
    "vgg_model.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# use cpu:\n",
    "\n",
    "# make labels one-hot encoded\n",
    "\n",
    "labels = tf.keras.utils.to_categorical(labels.reshape(-1), num_classes=1000)\n",
    "\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    vgg_model.fit(images, labels, epochs=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([490, 361, 171, ..., 128, 848, 186])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify a couple examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 00:08:15.524165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2023-03-21 00:08:15.953789: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 11.0.221, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-03-21 00:08:15.954676: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.6\n",
      "2023-03-21 00:08:15.954688: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:237] Used ptxas at ptxas\n",
      "2023-03-21 00:08:15.954740: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:318] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2023-03-21 00:08:16.629004: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.46GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-03-21 00:08:16.629027: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.46GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3/32 [=>............................] - ETA: 1s  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 00:08:22.741312: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 14s 194ms/step\n"
     ]
    }
   ],
   "source": [
    "y_hat = vgg_model.predict(images[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arg_max \n",
    "y_hat = np.argmax(y_hat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 65, 795, 160, 470, 431,  65, 334, 999, 532, 332, 109, 286, 370,\n",
       "       757, 840, 147, 552,  21, 526, 517, 334, 173, 679, 727,  86, 526,\n",
       "       270, 208,  64, 448, 324, 573, 150, 421, 586, 887, 113, 398, 529,\n",
       "       307, 431, 713, 129, 198, 222, 846, 565, 246, 817, 697,  21,  29,\n",
       "       844, 591, 333, 468, 157, 356, 840, 446, 735, 209, 107,  46, 522,\n",
       "       390, 101, 591, 870, 903,   6,   3,  21, 476,  80, 415, 178, 246,\n",
       "       170, 461, 970, 160, 788,  17, 882, 498, 376,  58, 487,  50, 222,\n",
       "       382, 366, 484, 373, 872, 330, 142, 956, 349, 473, 178, 872, 810,\n",
       "       552, 906,  70, 889, 632, 655, 887, 648, 227, 845, 876, 959, 516,\n",
       "       646, 480, 645, 449, 483, 714, 529, 309, 752, 352,  21, 934, 283,\n",
       "       802, 526, 276, 168, 526, 104, 772, 328, 969, 212, 178, 328, 771,\n",
       "       896, 977, 971, 267, 745, 590, 978, 624, 637,  39, 115, 108, 272,\n",
       "       279, 763, 765, 646, 205, 894, 438, 504, 937, 687, 781, 666, 583,\n",
       "       158, 825, 212, 659, 257, 436, 199, 142, 250, 203, 231, 617, 861,\n",
       "       961, 716, 627, 285, 908, 270,  58, 473, 178, 703, 403, 771, 119,\n",
       "       396, 916,  17, 548,   8, 704, 414, 233, 543, 481, 379, 138, 606,\n",
       "       922, 516, 284, 570, 475, 723, 818, 434,  16,  77, 610, 178, 636,\n",
       "       662, 473, 215, 971, 720, 257, 537, 947, 750, 669, 849, 289, 425,\n",
       "       973, 149, 166, 845, 562, 602, 532,   5, 570, 438, 862, 348,  13,\n",
       "       180, 731, 857, 989, 455, 647, 449, 570, 533, 761,  38, 644, 169,\n",
       "       729, 794, 172, 694, 695, 426, 809, 413, 257, 971, 821, 421, 529,\n",
       "       815, 526, 730, 691,  92, 219, 897, 612, 610, 283, 881, 457, 711,\n",
       "       426, 554, 895, 531, 697, 974, 669, 866, 738, 517,  51,  65, 999,\n",
       "       176, 112, 463, 390, 936, 591, 296,   1, 417, 606, 647, 572,   2,\n",
       "       979, 654, 905, 390, 677,  24, 102, 584, 508, 361, 272,  65, 777,\n",
       "       359, 208,  21, 904, 525, 894, 938, 254, 702, 707, 463,  61, 711,\n",
       "       477, 370, 851, 971, 355, 503, 818, 648, 472, 293, 816, 171, 905,\n",
       "       475, 481, 431, 283, 129, 627, 706, 622, 283, 300,  37, 133, 637,\n",
       "       867, 902, 662, 741, 895,  80, 109, 818, 694, 723, 210, 708, 424,\n",
       "       431, 209, 879, 643, 649, 228, 621, 703, 107, 827, 420, 975, 133,\n",
       "       190, 471, 230, 974, 319,  76, 720, 562, 267, 906, 359, 871, 510,\n",
       "       725, 177, 463, 310, 998, 270, 669, 589, 998, 523,  92,  97, 775,\n",
       "       398, 319, 549,  92, 765, 412, 945, 160, 265, 639, 846, 722, 183,\n",
       "       674, 734, 864, 748, 675, 636, 183, 677, 721,  20, 199, 209, 804,\n",
       "       946, 350, 728, 277, 361, 545, 861, 208, 606, 569, 767, 746, 788,\n",
       "       386, 203, 739, 797, 915, 793, 157, 470, 849, 898, 100, 546, 657,\n",
       "         6, 141,  39, 109, 613, 190, 888, 330, 584, 753, 723, 356, 388,\n",
       "       894, 437, 529, 517, 372, 279, 662, 713, 915, 666, 296, 529, 416,\n",
       "       374, 146, 902, 394, 398, 170, 270, 335, 680, 540, 530, 495, 222,\n",
       "       115, 576,  94, 178,  56, 818, 448, 908, 796, 278,   4, 651, 753,\n",
       "       795, 832,  39, 181, 863, 711, 411, 331, 983, 222, 180, 781, 223,\n",
       "       635, 723, 269, 491, 896, 267, 680,  48, 729, 750, 820, 148, 113,\n",
       "        99, 146, 691, 796, 309, 346, 274, 584, 423, 536, 485, 632, 812,\n",
       "       705, 409, 466, 781, 784, 617, 679, 858, 795, 437,  14, 536, 728,\n",
       "       874, 680, 600,  86, 133,  89, 723, 582, 309, 835, 834, 506, 279,\n",
       "       900, 905, 229, 410, 257, 424, 295, 132, 161, 833, 730, 386, 680,\n",
       "       178, 288, 660, 850, 264, 269, 108, 728, 611, 641, 147, 288, 802,\n",
       "       203, 156, 643, 775, 111, 750, 309, 711, 618, 657, 499, 833, 551,\n",
       "       301, 437, 415, 102, 791, 959, 404, 971, 424, 455, 172, 600, 657,\n",
       "        16, 318, 632, 501, 298, 222,   6, 726, 712, 605, 132, 677, 288,\n",
       "        20, 580, 719, 516, 273, 971, 267, 261, 898, 526, 172, 579, 450,\n",
       "       694, 217, 526, 677, 169, 126, 698, 751, 258, 513, 876, 336, 733,\n",
       "       916, 794,  78, 946, 804, 746,  39, 618, 331, 615, 756,  62, 116,\n",
       "       127, 955, 316, 449, 190, 381, 192, 971, 412, 396, 657, 840, 718,\n",
       "       116, 424, 994, 999, 886, 811, 285, 641, 723, 648,  13, 829, 624,\n",
       "       216, 661, 712, 268, 468, 418, 728, 330, 449, 194, 362, 774, 712,\n",
       "       248, 794, 936, 480, 283, 916, 129, 486, 859, 498,  21, 617, 434,\n",
       "       924, 765,  97, 591, 586,  17, 266,  68, 649,  17, 603, 727, 101,\n",
       "       584, 278, 725, 161, 669, 489, 434, 624, 451, 569, 732, 514, 586,\n",
       "       815, 551, 753, 411,  74, 861, 129, 389, 406, 526, 839, 299, 431,\n",
       "       112, 107, 632, 675, 227,  61, 427, 367, 924, 971, 125, 773, 654,\n",
       "       131, 874, 285, 891, 818, 285, 233, 631, 226, 673, 423, 907, 770,\n",
       "        38, 691, 794, 309, 728, 663, 782, 687, 818, 483, 801, 308,  78,\n",
       "       107, 526, 904, 711, 222, 138, 725, 620, 368, 586, 916, 478, 799,\n",
       "       829, 845, 590, 971,  19, 171, 915, 730, 917, 822, 771, 699, 406,\n",
       "       673, 196, 921, 624, 679, 667, 581, 665, 708, 818, 620, 800, 804,\n",
       "       669,  81, 746, 866, 412, 668, 294, 893, 279, 628, 470, 801, 229,\n",
       "       727, 846, 861, 203, 589,  70, 129, 582,  74, 946, 153, 181, 163,\n",
       "        86, 773, 438,  21, 198,  65, 534, 246, 480, 558, 883, 495, 781,\n",
       "       331, 462, 614, 554, 591, 919, 918, 479, 452, 155,  41, 242, 606,\n",
       "         8, 270, 898, 716, 506,  23, 982, 801, 730, 753, 107, 203, 879,\n",
       "       305, 318, 723, 530, 927, 539, 109, 713, 406,  65,  76, 517, 806,\n",
       "       331, 982,  88,  58,  56, 746,  78,  31, 373, 227, 793, 177, 153,\n",
       "       793, 888,  96, 870, 973, 745, 302, 819, 171, 818, 694, 406,  20,\n",
       "        18, 371, 310, 780,   5, 730, 662, 105, 810, 311, 883, 367, 243,\n",
       "       691, 680, 999, 412, 692, 332, 445, 106, 845, 917, 120, 723, 556,\n",
       "       562, 328, 723, 161, 248,   1, 919, 109, 264, 408, 471, 495])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indecies of labels where label = 1:\n",
    "indeces = np.where(labels == 23)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2027,  2306,  2453,  5148,  5293,  5620,  7869,  8478, 12257,\n",
       "       14436, 14832, 15698, 16648, 22123, 22235, 23993, 24373, 25531,\n",
       "       26105, 27303, 27994, 28457, 29077, 29497, 29733, 29821, 30315,\n",
       "       30623, 31890, 33046, 34451, 35242, 36870, 37446, 39408, 40697,\n",
       "       41086, 41632, 43520, 46149, 46174, 46218, 46402, 48095, 48216,\n",
       "       48717, 49512, 49544, 49779, 49991])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a numpy array of the indeceis of the images that are of class 23:\n",
    "indeces = np.where(labels == 490)[0]\n",
    "\n",
    "# get the images that are of class 23:\n",
    "images_23 = images[indeces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 224, 224, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_23.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 00:52:08.185513: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8100\n",
      "2023-03-21 00:52:08.605124: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:115] *** WARNING *** You are using ptxas 11.0.221, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n",
      "2023-03-21 00:52:08.605882: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:234] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 8.6\n",
      "2023-03-21 00:52:08.605889: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:237] Used ptxas at ptxas\n",
      "2023-03-21 00:52:08.605918: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:318] UNIMPLEMENTED: ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2023-03-21 00:52:09.266399: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.46GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-03-21 00:52:09.266423: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.46GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/2 [==============>...............] - ETA: 7s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-21 00:52:15.497964: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 12s 5s/step\n"
     ]
    }
   ],
   "source": [
    "y_hat = vgg_model.predict(images_23)\n",
    "\n",
    "# y_hat = np.argmax(y_hat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.08049573e-06, 1.39582619e-06, 5.39132679e-06, 5.26597723e-05,\n",
       "       8.17847904e-04, 2.40933237e-04, 1.55963795e-03, 4.07961625e-06,\n",
       "       4.32922121e-07, 2.11239574e-04, 2.39513292e-06, 6.49916672e-07,\n",
       "       9.32556702e-07, 1.14248296e-05, 8.80197149e-07, 1.71315651e-06,\n",
       "       2.51105962e-06, 8.02282045e-07, 1.89297225e-06, 9.62175818e-07,\n",
       "       3.56643091e-06, 5.40468045e-06, 6.03617991e-06, 3.32983473e-06,\n",
       "       5.95494555e-07, 3.49840229e-05, 3.20794265e-04, 7.18044685e-05,\n",
       "       4.05569517e-05, 6.92980393e-05, 1.51267901e-04, 2.67412065e-06,\n",
       "       6.28409398e-05, 1.87684281e-03, 7.42064673e-04, 2.46094278e-05,\n",
       "       5.56699488e-05, 1.22243900e-05, 7.89894024e-04, 1.06633146e-04,\n",
       "       1.56264232e-05, 7.86623568e-04, 3.55598277e-05, 6.74992916e-05,\n",
       "       6.01565873e-04, 1.75298046e-04, 4.30785076e-05, 9.27307556e-05,\n",
       "       1.16038986e-03, 3.88577208e-03, 3.70594906e-04, 8.62703109e-06,\n",
       "       1.96443629e-02, 2.68520936e-02, 2.82011610e-02, 3.38141137e-04,\n",
       "       2.45115487e-03, 8.22421105e-04, 1.81943789e-01, 1.49096362e-03,\n",
       "       2.30350457e-02, 8.25169683e-03, 4.68330681e-02, 5.45346364e-02,\n",
       "       2.00376194e-03, 4.76723820e-01, 2.16709170e-02, 1.47761172e-02,\n",
       "       1.67912915e-02, 2.24366286e-05, 1.12701719e-06, 4.02147503e-04,\n",
       "       2.53388976e-07, 1.70080409e-06, 4.01192466e-07, 1.98297812e-06,\n",
       "       4.58012619e-06, 5.70166821e-06, 1.53608271e-04, 2.47153512e-04,\n",
       "       3.30942476e-06, 2.70029068e-06, 1.00582711e-05, 3.00580882e-06,\n",
       "       2.42217484e-05, 2.88639258e-05, 3.75149671e-06, 2.07778430e-06,\n",
       "       6.36929315e-07, 2.80148947e-06, 4.19383269e-07, 3.66431027e-06,\n",
       "       8.10510392e-06, 1.35266055e-06, 7.64860602e-07, 3.00290253e-07,\n",
       "       1.48585343e-06, 6.94532855e-06, 5.05039388e-05, 1.65507390e-05,\n",
       "       5.00747483e-05, 5.16488253e-05, 1.62688957e-05, 1.01341000e-04,\n",
       "       3.51112220e-04, 3.55942780e-06, 1.57652121e-05, 8.08822078e-05,\n",
       "       1.73977387e-05, 1.75694931e-05, 5.50023906e-05, 2.08359864e-03,\n",
       "       3.46183369e-04, 4.97777437e-05, 2.18042842e-04, 2.48408742e-06,\n",
       "       1.23283417e-05, 2.61703317e-05, 1.82533440e-05, 2.46961863e-05,\n",
       "       3.66965774e-04, 2.36130791e-06, 5.17954959e-06, 1.34421941e-06,\n",
       "       5.32089616e-05, 2.03618431e-04, 1.95211669e-05, 1.09334826e-06,\n",
       "       8.58170279e-06, 1.22378515e-05, 3.90350942e-05, 1.00072699e-04,\n",
       "       5.73626603e-05, 4.17737738e-06, 1.88127524e-05, 1.05223762e-05,\n",
       "       7.67882000e-07, 7.47053264e-06, 1.48267081e-05, 4.09328131e-05,\n",
       "       5.75077029e-05, 3.30674193e-05, 8.33665617e-05, 3.61940838e-05,\n",
       "       2.36089036e-05, 4.85300143e-06, 1.18426815e-05, 1.57681152e-05,\n",
       "       1.06153884e-05, 5.12173792e-05, 5.48164942e-04, 1.14156928e-05,\n",
       "       2.06770142e-06, 1.01276410e-05, 4.63626793e-06, 5.01255590e-06,\n",
       "       2.12056930e-05, 7.94238804e-06, 7.36521906e-06, 2.96743790e-04,\n",
       "       6.67887798e-05, 7.74185173e-05, 4.67902755e-05, 3.26018402e-04,\n",
       "       6.24576642e-05, 8.26632549e-06, 3.96097057e-05, 1.26776631e-05,\n",
       "       1.88631559e-04, 2.09907448e-05, 1.11999281e-04, 7.19604941e-05,\n",
       "       7.75938039e-04, 2.76941952e-04, 1.49157577e-05, 2.38230423e-05,\n",
       "       5.57488762e-04, 8.70204822e-05, 8.39572967e-05, 3.68839137e-05,\n",
       "       2.36013839e-05, 1.68902316e-05, 2.93949543e-05, 6.25732355e-06,\n",
       "       4.18707968e-05, 3.66541681e-05, 8.62694924e-06, 5.41215923e-06,\n",
       "       9.21083665e-06, 3.02826174e-05, 3.73748799e-06, 4.53396206e-05,\n",
       "       7.86397413e-06, 6.63648098e-06, 5.92476090e-06, 1.85374138e-05,\n",
       "       4.57274973e-06, 3.97342556e-05, 6.35859669e-06, 1.81141804e-05,\n",
       "       1.42892932e-05, 3.03561615e-06, 5.85515263e-05, 2.50527683e-05,\n",
       "       1.26459654e-05, 2.74182457e-05, 3.49009497e-05, 5.78021391e-05,\n",
       "       5.17874723e-05, 1.97066955e-04, 6.57677374e-05, 5.91519456e-05,\n",
       "       3.72414761e-05, 5.30751677e-05, 2.14614302e-05, 1.67813723e-05,\n",
       "       2.80602717e-06, 6.57688652e-05, 4.27887862e-05, 1.30362714e-05,\n",
       "       1.57126178e-05, 3.43828360e-05, 2.27348373e-05, 3.15947327e-05,\n",
       "       2.41605921e-05, 5.16808541e-05, 3.42598032e-05, 2.06948476e-04,\n",
       "       4.91865740e-06, 6.45455220e-05, 4.35099682e-05, 4.80582385e-05,\n",
       "       3.13330529e-05, 2.86821651e-05, 3.37455422e-05, 1.08820946e-04,\n",
       "       7.12836045e-05, 2.07061094e-05, 1.77249414e-04, 2.80899512e-05,\n",
       "       1.35618357e-05, 5.96244136e-05, 5.08177873e-05, 3.27351081e-05,\n",
       "       3.15891302e-05, 1.30750695e-05, 1.38739357e-04, 4.56435664e-05,\n",
       "       2.49659297e-05, 4.42797391e-05, 7.63413336e-05, 8.80517473e-06,\n",
       "       1.45843251e-06, 2.41319231e-05, 1.42680638e-05, 5.10459040e-06,\n",
       "       6.44070460e-05, 1.83382799e-05, 3.79308949e-05, 4.89808508e-06,\n",
       "       1.29207065e-05, 7.19692343e-06, 4.77064168e-06, 1.93382803e-05,\n",
       "       6.36733166e-05, 2.08591291e-06, 1.87214823e-06, 5.97390135e-06,\n",
       "       1.64839021e-05, 1.18644775e-05, 2.25034000e-05, 6.64805384e-06,\n",
       "       1.91632844e-05, 1.86168938e-03, 2.68757485e-05, 2.42938113e-05,\n",
       "       1.85008656e-04, 1.09975608e-05, 8.40872235e-06, 4.14848510e-05,\n",
       "       2.05850811e-05, 7.94850348e-06, 4.20735932e-06, 2.12693203e-06,\n",
       "       1.48112331e-05, 1.86679863e-05, 2.60283086e-05, 1.60212367e-05,\n",
       "       2.93169869e-05, 3.34796573e-06, 6.23816777e-06, 7.23118937e-05,\n",
       "       2.08482597e-05, 6.16476536e-05, 1.17661421e-05, 5.54546887e-06,\n",
       "       1.42216168e-05, 4.71940757e-06, 1.61174088e-04, 1.36322606e-05,\n",
       "       2.56699423e-06, 4.25832286e-06, 3.24109078e-06, 1.08749464e-06,\n",
       "       1.04502385e-06, 1.58777199e-04, 6.54746109e-05, 4.09420045e-06,\n",
       "       2.74659919e-06, 7.40969256e-07, 3.75869713e-05, 3.24657253e-06,\n",
       "       8.31226316e-06, 2.04297066e-06, 1.21863915e-04, 4.20674132e-06,\n",
       "       2.13520752e-06, 1.29203056e-06, 2.46111194e-05, 4.32029401e-06,\n",
       "       1.65854919e-06, 5.43831050e-07, 1.72495120e-07, 2.56328150e-07,\n",
       "       1.04032587e-07, 3.94143626e-07, 4.19188126e-07, 7.12051988e-04,\n",
       "       6.02343698e-06, 2.64240545e-04, 5.86692986e-05, 1.92026182e-05,\n",
       "       1.34746153e-06, 1.23972950e-06, 1.45837757e-05, 4.38806092e-06,\n",
       "       2.53659869e-06, 9.87525200e-05, 3.28947522e-06, 6.90133493e-06,\n",
       "       1.64693192e-05, 1.42058252e-05, 2.59050103e-05, 1.05040643e-04,\n",
       "       1.38942749e-04, 3.85957042e-04, 3.99017381e-03, 5.98705046e-05,\n",
       "       3.35033246e-06, 2.06074528e-05, 8.59982174e-05, 1.23264559e-04,\n",
       "       1.28784421e-04, 1.66835307e-04, 1.36104311e-04, 4.13871066e-06,\n",
       "       1.51551412e-05, 2.76603132e-05, 1.60173586e-05, 9.90063290e-06,\n",
       "       1.44174250e-04, 1.80300740e-05, 6.29895294e-06, 1.26945815e-04,\n",
       "       2.43568979e-06, 5.57867952e-06, 4.13826456e-06, 9.81916492e-06,\n",
       "       1.51050358e-06, 4.80358085e-06, 3.58183070e-06, 2.07483517e-05,\n",
       "       2.31658487e-05, 1.85274130e-05, 6.31614821e-05, 4.40101940e-06,\n",
       "       1.01958940e-05, 3.24005237e-06, 1.50050437e-05, 5.80189362e-06,\n",
       "       1.50743899e-05, 1.46357306e-05, 3.93633354e-06, 6.14597093e-06,\n",
       "       2.61864466e-06, 2.05503326e-04, 8.32920923e-05, 1.53086603e-06,\n",
       "       1.33999436e-06, 5.25362266e-05, 4.21129353e-03, 1.40833163e-05,\n",
       "       1.79308142e-06, 4.81331153e-06, 8.63816604e-05, 3.10938864e-04,\n",
       "       2.09802183e-05, 1.07169653e-05, 1.26373618e-06, 1.34557204e-05,\n",
       "       1.04413964e-07, 1.74979004e-06, 4.65914809e-06, 4.10950179e-07,\n",
       "       2.22066501e-06, 2.39822998e-06, 3.81690086e-07, 2.66057486e-06,\n",
       "       4.25115104e-05, 1.08584018e-05, 3.82993085e-06, 4.37827310e-07,\n",
       "       2.76761966e-05, 7.12721430e-06, 8.86886653e-07, 2.05209672e-05,\n",
       "       6.21033996e-06, 4.26727820e-06, 4.88752039e-06, 2.00132017e-05,\n",
       "       2.59038261e-05, 1.85855208e-06, 2.46491472e-05, 9.18179830e-06,\n",
       "       3.61507750e-06, 6.05676803e-07, 4.28877502e-06, 2.36008000e-05,\n",
       "       2.69448501e-04, 1.62384349e-05, 2.56110343e-06, 8.46853254e-06,\n",
       "       1.21523008e-05, 1.66611193e-04, 3.13241144e-05, 8.57979321e-05,\n",
       "       7.39547750e-06, 9.37295317e-06, 4.63600281e-06, 2.55265547e-07,\n",
       "       1.29213959e-05, 8.70756503e-06, 1.49804919e-07, 2.28645804e-05,\n",
       "       1.86277593e-05, 2.63876544e-04, 1.19572233e-05, 8.78273204e-06,\n",
       "       1.55469638e-06, 3.72966133e-06, 2.23403986e-05, 1.93789060e-06,\n",
       "       2.55290729e-06, 4.25809139e-06, 6.05605237e-07, 1.81562409e-05,\n",
       "       1.75998484e-05, 4.89529896e-07, 5.28686269e-06, 2.71241083e-06,\n",
       "       2.28323752e-05, 4.49495559e-07, 3.85189232e-05, 1.24183833e-04,\n",
       "       1.89358082e-06, 7.68151551e-07, 2.21880086e-07, 1.19657273e-06,\n",
       "       1.07541439e-06, 4.60115371e-06, 1.06767411e-05, 3.68951896e-06,\n",
       "       1.82517961e-05, 6.82713799e-05, 3.34371862e-06, 1.10037090e-05,\n",
       "       1.25588235e-06, 1.05304127e-06, 6.92135927e-06, 3.69792383e-06,\n",
       "       1.34834329e-06, 3.81453378e-06, 3.43912166e-06, 2.09532615e-07,\n",
       "       2.40381833e-05, 3.96792211e-06, 2.28331692e-06, 6.82464088e-06,\n",
       "       3.04416579e-04, 2.22586800e-06, 5.49248853e-06, 1.29771533e-05,\n",
       "       7.27739280e-06, 4.59348121e-06, 8.23104017e-07, 2.88588217e-06,\n",
       "       5.37318954e-07, 2.37555241e-07, 7.75592071e-07, 3.97224494e-06,\n",
       "       3.34569017e-06, 1.31045601e-06, 7.27379074e-06, 4.12482905e-06,\n",
       "       7.29636440e-06, 8.59493048e-06, 5.13911982e-06, 7.70847328e-06,\n",
       "       7.46187379e-06, 2.63931241e-07, 2.03380969e-05, 4.37281778e-06,\n",
       "       2.15661694e-05, 5.28334431e-06, 2.36051874e-05, 3.24837347e-05,\n",
       "       9.16977478e-06, 8.36797972e-06, 1.99196029e-06, 6.14457622e-06,\n",
       "       6.80180210e-06, 1.03921329e-05, 6.82057453e-06, 2.43252271e-05,\n",
       "       2.28692230e-07, 8.20717651e-06, 8.82302902e-06, 1.35722053e-06,\n",
       "       4.23127931e-05, 8.88368013e-05, 4.22823541e-06, 3.17244280e-06,\n",
       "       2.31537015e-05, 3.65498977e-07, 3.44048954e-06, 2.11791667e-06,\n",
       "       2.46424111e-06, 3.46162415e-05, 2.25366989e-06, 4.56635098e-05,\n",
       "       3.65952292e-06, 6.37162293e-06, 4.29004576e-05, 4.49285744e-06,\n",
       "       8.17116961e-05, 5.70504062e-06, 7.50470781e-06, 4.57882209e-07,\n",
       "       2.57100464e-06, 9.60739926e-05, 4.24529219e-07, 2.21447917e-04,\n",
       "       8.56014424e-07, 2.91157303e-05, 1.93108690e-06, 1.48215531e-06,\n",
       "       1.86020552e-05, 8.73475346e-06, 1.94116910e-05, 6.48801642e-06,\n",
       "       4.73862656e-07, 9.79860306e-06, 7.29264229e-06, 2.84153339e-06,\n",
       "       1.00213319e-05, 1.03887385e-06, 2.73687760e-06, 1.25434735e-05,\n",
       "       1.75105879e-06, 1.83376960e-06, 7.63942444e-06, 5.15056126e-06,\n",
       "       1.62674205e-05, 4.97588990e-05, 7.08623065e-06, 4.50715088e-05,\n",
       "       5.48943035e-06, 1.06827720e-05, 1.98332418e-06, 2.16191029e-06,\n",
       "       1.67716689e-06, 2.65853174e-07, 3.98967495e-06, 9.55318342e-07,\n",
       "       1.68551435e-06, 6.48371179e-06, 7.42233824e-05, 2.65798190e-05,\n",
       "       9.85399492e-06, 1.09331022e-05, 1.46352079e-06, 1.78454411e-05,\n",
       "       1.63014238e-06, 3.33905973e-06, 4.30041327e-05, 3.18868251e-06,\n",
       "       4.13871676e-05, 1.56388489e-06, 5.62797004e-06, 4.62313710e-06,\n",
       "       7.80011585e-04, 8.48500292e-07, 2.28446470e-06, 1.26808918e-05,\n",
       "       4.26249107e-06, 3.04076639e-05, 8.70377480e-05, 5.34425908e-07,\n",
       "       1.32065543e-05, 4.46928789e-05, 8.37137213e-06, 3.81713035e-05,\n",
       "       2.35580160e-06, 2.36131700e-06, 2.78700497e-07, 3.86214606e-06,\n",
       "       1.27274820e-04, 4.76625792e-06, 1.48648003e-04, 2.09442032e-06,\n",
       "       7.21258948e-06, 2.56525782e-05, 7.59547402e-06, 3.24023931e-05,\n",
       "       7.42590601e-07, 4.16234934e-06, 1.14598579e-05, 1.78082689e-06,\n",
       "       2.60651836e-06, 9.27389374e-06, 6.76359923e-06, 8.70467120e-05,\n",
       "       1.60701200e-06, 1.83561115e-05, 2.00564295e-06, 2.99041076e-05,\n",
       "       1.94092195e-06, 7.60036482e-06, 1.76344154e-04, 3.53763433e-04,\n",
       "       1.42376302e-05, 4.21652976e-06, 3.19664150e-06, 1.35839241e-06,\n",
       "       1.16775964e-05, 2.17448223e-06, 3.47522378e-04, 8.26345695e-06,\n",
       "       2.74524541e-06, 1.72491764e-06, 3.57372028e-06, 5.57356907e-06,\n",
       "       6.43736030e-06, 1.58894509e-05, 6.35377137e-06, 1.39553813e-05,\n",
       "       3.31368210e-06, 1.48665222e-05, 7.59693307e-07, 8.84196379e-06,\n",
       "       2.22188828e-05, 7.32175704e-06, 4.91331275e-06, 1.21852520e-06,\n",
       "       3.77927267e-06, 4.26833903e-05, 2.34276795e-05, 8.20860521e-07,\n",
       "       3.21040784e-06, 2.15503387e-05, 3.13220844e-05, 2.62217964e-05,\n",
       "       1.79394792e-05, 1.80718016e-05, 1.39773447e-05, 1.31447757e-06,\n",
       "       4.33708328e-06, 3.83556908e-05, 5.92319066e-06, 1.97793997e-05,\n",
       "       3.30031144e-05, 8.66706887e-06, 2.12536065e-06, 3.90179184e-06,\n",
       "       7.04774811e-06, 2.02960427e-06, 7.79176207e-06, 1.05757557e-07,\n",
       "       5.90009859e-06, 2.52148965e-07, 1.94131735e-05, 2.60204924e-05,\n",
       "       2.13937824e-06, 2.02443807e-05, 5.63216020e-07, 1.14093791e-05,\n",
       "       6.28572452e-05, 5.22258324e-06, 2.11025144e-06, 2.81882649e-06,\n",
       "       5.64347865e-05, 4.97020810e-05, 3.98474685e-06, 1.82351268e-05,\n",
       "       6.53494453e-06, 8.39135055e-07, 2.00993127e-05, 4.23993788e-06,\n",
       "       3.45255557e-06, 9.44300268e-07, 5.77342917e-06, 1.36071794e-05,\n",
       "       4.84832026e-06, 3.46522233e-06, 1.19621282e-05, 1.83624752e-06,\n",
       "       3.06267975e-06, 1.56175213e-06, 2.61741138e-06, 7.01689805e-06,\n",
       "       1.41053733e-05, 5.76053753e-06, 5.24331881e-06, 3.95876128e-07,\n",
       "       1.10408723e-06, 6.69541078e-06, 9.81249923e-06, 7.86551482e-07,\n",
       "       1.14115176e-06, 3.73703142e-06, 3.70310299e-05, 3.67162320e-05,\n",
       "       1.00198076e-06, 2.44345738e-05, 6.88868738e-07, 5.02255432e-07,\n",
       "       3.48973686e-06, 3.74684851e-06, 1.14707927e-05, 1.91349420e-04,\n",
       "       1.78562859e-05, 1.92703465e-05, 8.91757395e-07, 7.55404130e-07,\n",
       "       6.22770585e-06, 1.95783014e-06, 6.51984883e-04, 2.21862661e-06,\n",
       "       1.75469802e-06, 1.20097538e-04, 8.45996783e-06, 1.75833884e-05,\n",
       "       1.69857449e-05, 1.57876366e-06, 3.08967537e-06, 3.75906052e-06,\n",
       "       4.91697610e-05, 2.48914203e-05, 3.36717021e-05, 1.04968728e-06,\n",
       "       1.32004971e-05, 1.76929807e-05, 3.36721655e-06, 1.96374967e-05,\n",
       "       1.19428470e-04, 5.50449704e-06, 2.56424244e-07, 1.14145269e-05,\n",
       "       5.30993384e-06, 2.56660369e-05, 4.66054553e-05, 5.68336736e-06,\n",
       "       2.04508870e-05, 5.45823923e-06, 8.18130648e-05, 1.82313361e-05,\n",
       "       1.21423845e-05, 1.08428947e-06, 1.84745859e-05, 1.86980708e-06,\n",
       "       8.70675922e-06, 6.40032852e-07, 4.34138701e-06, 1.64253706e-05,\n",
       "       6.19978300e-06, 8.00648650e-07, 6.19152252e-06, 3.86860802e-06,\n",
       "       2.98664736e-06, 6.86598423e-06, 1.68353210e-06, 4.81040006e-05,\n",
       "       2.74020655e-04, 1.58447965e-05, 6.69958808e-06, 5.30830657e-05,\n",
       "       7.05027560e-06, 5.92662627e-06, 1.81026167e-06, 2.42192782e-05,\n",
       "       1.53727470e-07, 5.29352168e-04, 1.01442238e-04, 5.27986913e-06,\n",
       "       7.20869730e-06, 9.91177585e-05, 6.16066473e-06, 3.16896831e-06,\n",
       "       1.90788724e-05, 1.96818910e-05, 1.02333456e-06, 5.19401328e-06,\n",
       "       3.36709945e-07, 5.60968647e-06, 1.07226388e-05, 5.14632632e-07,\n",
       "       1.11586842e-05, 7.53900736e-07, 2.02161823e-06, 4.06525572e-07,\n",
       "       6.36765321e-07, 6.11960218e-07, 4.78739548e-06, 7.86880191e-05,\n",
       "       2.31795366e-06, 1.74372315e-06, 2.25397634e-05, 1.33942649e-05,\n",
       "       1.48796998e-05, 9.94339757e-07, 2.81994089e-05, 1.33746462e-05,\n",
       "       1.09406062e-06, 1.31420247e-05, 9.42348947e-07, 1.73893477e-05,\n",
       "       2.06504792e-05, 2.99298717e-05, 3.88514396e-04, 4.78154391e-07,\n",
       "       4.84013844e-06, 1.54224165e-06, 3.57510668e-04, 1.10024339e-04,\n",
       "       3.18999810e-05, 1.69977357e-05, 4.16379044e-06, 3.53206706e-05,\n",
       "       3.06967127e-06, 7.87492354e-06, 1.05193806e-06, 3.04740411e-06,\n",
       "       1.91552626e-05, 7.36697928e-07, 2.82405580e-07, 1.24838634e-05,\n",
       "       3.92480661e-06, 9.34660648e-07, 6.50242464e-06, 3.55043335e-06,\n",
       "       5.82444272e-07, 2.69386455e-05, 6.06005017e-07, 6.99802285e-07,\n",
       "       4.52014456e-06, 1.71575380e-07, 4.84140619e-05, 4.01803027e-06,\n",
       "       8.78233550e-05, 5.07422840e-07, 2.30892925e-04, 3.06277911e-06,\n",
       "       3.22942219e-06, 9.15755322e-07, 1.43449938e-06, 4.08497726e-06,\n",
       "       1.42146804e-04, 3.54907087e-07, 1.05666982e-06, 1.01227743e-05,\n",
       "       8.12909711e-05, 1.24400970e-06, 7.42203410e-05, 5.98529505e-06,\n",
       "       9.99180997e-07, 3.10351356e-06, 1.16462593e-06, 4.74300776e-07,\n",
       "       4.18898026e-07, 6.74180001e-06, 1.65611520e-04, 5.72980071e-06,\n",
       "       7.12298151e-06, 2.52590485e-06, 5.92246761e-06, 3.93294522e-06,\n",
       "       6.94856499e-05, 1.22340125e-05, 2.03313484e-05, 3.03087072e-05,\n",
       "       4.60519828e-07, 1.22226647e-05, 1.48690324e-05, 3.90224932e-06,\n",
       "       5.43970873e-06, 3.40715815e-06, 2.11919257e-07, 7.43730652e-06,\n",
       "       7.04473723e-06, 3.06607581e-06, 8.33251470e-05, 1.20021389e-06,\n",
       "       6.94570599e-06, 4.43823483e-05, 1.05862109e-05, 6.12183476e-06,\n",
       "       1.69628081e-04, 1.15783687e-05, 6.43541134e-05, 8.76188369e-06,\n",
       "       6.82275072e-07, 6.25750545e-05, 5.31583973e-05, 9.97183452e-06,\n",
       "       1.04216940e-06, 9.84796407e-06, 1.77390257e-06, 7.22427831e-06,\n",
       "       3.21410835e-06, 1.43267771e-05, 2.20395737e-06, 9.47801345e-06,\n",
       "       1.93153155e-05, 1.70264286e-06, 1.22366728e-05, 5.46927049e-06,\n",
       "       1.32996206e-06, 6.44908198e-07, 1.72689511e-07, 2.70104556e-06,\n",
       "       1.35568632e-06, 1.92336915e-06, 1.08397680e-05, 6.70481040e-06,\n",
       "       7.87143506e-07, 1.93190817e-06, 2.69690787e-07, 4.19505886e-06,\n",
       "       6.33471245e-06, 7.41261829e-07, 1.68129532e-06, 2.89964532e-06,\n",
       "       1.39830979e-06, 3.58073947e-07, 5.39967778e-06, 4.09776749e-06,\n",
       "       2.56047338e-06, 1.26974135e-06, 3.68760375e-05, 6.51324854e-06,\n",
       "       8.04822866e-06, 1.44603167e-04, 2.78787047e-06, 4.29438660e-06,\n",
       "       3.44935324e-06, 1.40022996e-06, 1.57013837e-05, 7.97669145e-06,\n",
       "       3.59024561e-05, 1.86476336e-05, 6.60554360e-05, 4.65226367e-05,\n",
       "       7.14969337e-06, 4.38654024e-05, 1.56612383e-04, 2.89089279e-04,\n",
       "       1.57053819e-05, 1.30148213e-02, 3.73239792e-03, 1.44622409e-05,\n",
       "       7.78860522e-06, 7.30608417e-06, 1.12936266e-06, 8.88804061e-06,\n",
       "       2.59591593e-06, 1.79110486e-07, 2.33118635e-06, 5.46473202e-05,\n",
       "       1.38207097e-05, 3.30809144e-07, 4.48178707e-06, 7.45376155e-07,\n",
       "       2.30852592e-07, 2.11675979e-06, 7.87989279e-07, 1.77314155e-06,\n",
       "       3.41633381e-06, 3.98675047e-06, 7.02673515e-06, 7.45130001e-06],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
      "35363/35363 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('n01751748', 'sea_snake', 0.47672382)],\n",
       " [('n01751748', 'sea_snake', 0.78579056)],\n",
       " [('n01751748', 'sea_snake', 0.98491466)],\n",
       " [('n01751748', 'sea_snake', 0.62539023)],\n",
       " [('n01751748', 'sea_snake', 0.6275712)],\n",
       " [('n01751748', 'sea_snake', 0.4979992)],\n",
       " [('n01737021', 'water_snake', 0.67158806)],\n",
       " [('n09256479', 'coral_reef', 0.26914987)],\n",
       " [('n01751748', 'sea_snake', 0.97943753)],\n",
       " [('n01950731', 'sea_slug', 0.9578252)],\n",
       " [('n01751748', 'sea_snake', 0.2198501)],\n",
       " [('n01751748', 'sea_snake', 0.7796114)],\n",
       " [('n02641379', 'gar', 0.271905)],\n",
       " [('n01751748', 'sea_snake', 0.9725391)],\n",
       " [('n02869837', 'bonnet', 0.11049271)],\n",
       " [('n01494475', 'hammerhead', 0.17553638)],\n",
       " [('n15075141', 'toilet_tissue', 0.23398037)],\n",
       " [('n02276258', 'admiral', 0.21024698)],\n",
       " [('n01751748', 'sea_snake', 0.7825636)],\n",
       " [('n01751748', 'sea_snake', 0.6292295)],\n",
       " [('n09256479', 'coral_reef', 0.47323078)],\n",
       " [('n01751748', 'sea_snake', 0.94948965)],\n",
       " [('n01751748', 'sea_snake', 0.9619373)],\n",
       " [('n01737021', 'water_snake', 0.7957842)],\n",
       " [('n01751748', 'sea_snake', 0.9473423)],\n",
       " [('n01751748', 'sea_snake', 0.93209904)],\n",
       " [('n01737021', 'water_snake', 0.1795925)],\n",
       " [('n01665541', 'leatherback_turtle', 0.69429415)],\n",
       " [('n01751748', 'sea_snake', 0.79867)],\n",
       " [('n09256479', 'coral_reef', 0.3188094)],\n",
       " [('n01751748', 'sea_snake', 0.89148396)],\n",
       " [('n01751748', 'sea_snake', 0.6667359)],\n",
       " [('n01751748', 'sea_snake', 0.8460162)],\n",
       " [('n01930112', 'nematode', 0.94191515)],\n",
       " [('n01737021', 'water_snake', 0.27149948)],\n",
       " [('n01924916', 'flatworm', 0.30817696)],\n",
       " [('n01737021', 'water_snake', 0.20003168)],\n",
       " [('n01751748', 'sea_snake', 0.511398)],\n",
       " [('n01729977', 'green_snake', 0.45379522)],\n",
       " [('n02526121', 'eel', 0.34514406)],\n",
       " [('n01924916', 'flatworm', 0.617965)],\n",
       " [('n03291819', 'envelope', 0.44650063)],\n",
       " [('n01751748', 'sea_snake', 0.55518794)],\n",
       " [('n01751748', 'sea_snake', 0.93882364)],\n",
       " [('n03532672', 'hook', 0.5201767)],\n",
       " [('n01751748', 'sea_snake', 0.45805857)],\n",
       " [('n01629819', 'European_fire_salamander', 0.2615799)],\n",
       " [('n01751748', 'sea_snake', 0.8541898)],\n",
       " [('n01751748', 'sea_snake', 0.43296027)],\n",
       " [('n01924916', 'flatworm', 0.38804507)]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.applications.vgg19.decode_predictions(y_hat, top=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
